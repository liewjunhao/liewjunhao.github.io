<!DOCTYPE HTML>
<html lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Jun Hao Liew</title>
		<meta name="author" content="Jun Hao Liew">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="stylesheet.css">
		<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>"> -->
		<link href="" rel="icon" type="image/png" />
	</head>

	<body>
		<table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tbody>
				<tr style="padding:0px">
					<td style="padding:0px">
					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr style="padding:0px">
								<td style="padding:2.5%;width:63%;vertical-align:middle">
									<p class="name" style="text-align: center;">Jun Hao Liew</p>
									<p>
										I am a senior research scientist at TikTok/ByteDance Research. My research focus is controllable diffusion-based generative models. Before joining TikTok, I worked as a research fellow at NUS. Prior to that, I did my Ph.D. with <a href="https://cde.nus.edu.sg/ece/staff/ong-sim-heng/">A/P Sim-Heng Ong</a>, <a href="https://scholar.google.com.sg/citations?user=AhITXNUAAAAJ&hl=en">Dr. Wei Xiong</a> and <a href="https://sites.google.com/site/jshfeng/home">Dr. Jiashi Feng</a> in NUS.
										<!-- I also work closely with <a href="https://hanshuyan.github.io/">Dr. Hanshu Yan</a>, <a href="http://jeff95.me/">Dr. Jianfeng Zhang</a> and <a href="https://weiyc.github.io/">Professor Yunchao Wei</a>.									 -->
									</p>
									<p>I am actively looking for research interns and collaborators. Please feel free to drop me an email if you are interested.</p>
									<p style="text-align:center">
									<a href="mailto:junhao.liew@bytedance.com">Email</a> &nbsp;/&nbsp;
									<a href="https://scholar.google.com/citations?user=8gm-CYYAAAAJ">Google Scholar</a> &nbsp;/&nbsp;
									<a href="https://sg.linkedin.com/in/liewjunhao">LinkedIn</a> &nbsp;/&nbsp;
									<a href="https://twitter.com/jhliew91">Twitter</a> &nbsp;/&nbsp;
									<a href="https://github.com/liewjunhao/">Github</a>
									</p>
								</td>
								<td style="padding:2.5%;width:40%;max-width:40%">
									<a href="images/liewjunhao.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/liewjunhao.png" class="hoverZoomLink"></a>
								</td>
							</tr>
						</tbody>
					</table>

					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>
							<tr>
								<td style="padding:20px;width:100%;vertical-align:middle">
								<h2>Research</h2>
								</td>
							</tr>
						</tbody>
					</table>

					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tbody>

							<!-- <tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/xxx.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="">
										<span class="papertitle">XXX</span>
									</a>
									<br>
									<strong>Jun Hao Liew</strong>,
									<br>
									<em>XXX</em>, 2023
									<br>
									<a href="">project page</a> /
									<a href="">code</a> /
									<a href="">arXiv</a>
									<p></p>
									<p>
										XXX
									</p>
								</td>
							</tr> -->

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/gigatok.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="">
										<span class="papertitle">GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for Autoregressive Image Generation</span>
									</a>
									<br>
									Tianwei Xiong,
									<strong>Jun Hao Liew</strong>,
									Zilong Huang,
									Jiashi Feng,
									Xihui Liu
									<br>
									<em>ICCV</em>, 2025
									<br>
									<a href="https://silentview.github.io/GigaTok/">project page</a> /
									<a href="https://github.com/SilentView/GigaTok">code</a> /
									<a href="https://arxiv.org/abs/2504.08736">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/sail.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="">
										<span class="papertitle">The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</span>
									</a>
									<br>
									Weixian Lei*,
									Jiacong Wang*,
									Haochen Wang*,
									Xiangtai Li,
									<strong>Jun Hao Liew</strong>,
									<br>
									Jiashi Feng,
									Zilong Huang
									<br>
									<em>ICCV</em>, 2025
									<br>
									<a href="https://github.com/ByteDance-Seed/SAIL">code</a> /
									<a href="https://arxiv.org/abs/2504.10462">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/lightningdrag.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://lightning-drag.github.io/">
										<span class="papertitle">LightningDrag: Lightning Fast and Accurate Drag-based Image Editing Emerging from Videos</span>
									</a>
									<br>
									Yujun Shi*,
									<strong>Jun Hao Liew*</strong>,
									Hanshu Yan,
									Vincent Y. F. Tan,
									Jiashi Feng
									<br>
									<em>ICML</em>, 2025
									<br>
									<a href="https://lightning-drag.github.io/">project page</a> /
									<a href="https://github.com/magic-research/LightningDrag">code</a> /
									<a href="https://arxiv.org/abs/2405.13722">arXiv</a> /
									<a href="https://huggingface.co/spaces/LightningDrag/LightningDrag">HuggingFace demo</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/dig.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2405.18428">
										<span class="papertitle">DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention</span>
									</a>
									<br>
									Lianghui Zhu,
									Zilong Huang,
									Bencheng Liao,
									<strong>Jun Hao Liew</strong>,
									Hanshu Yan,
									Jiashi Feng,
									Xinggang Wang
									<br>
									<em>CVPR</em>, 2025
									<br>
									<a href="https://github.com/hustvl/DiG">code</a> /
									<a href="https://arxiv.org/abs/2405.18428">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicarticulate.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://chaoyuesong.github.io/MagicArticulate/">
										<span class="papertitle">MagicArticulate: Make Your 3D Models Articulation-Ready</span>
									</a>
									<br>
									Chaoyue Song,
									Jianfeng Zhang,
									Xiu Li,
									Fan Yang,
									Yiwen Chen,
									Zhongcong Xu,
									<br>
									<strong>Jun Hao Liew</strong>,
									Xiaoyang Guo,
									Fayao Liu,
									Jiashi Feng,
									Guosheng Lin
									<br>
									<em>CVPR</em>, 2025
									<br>
									<a href="https://chaoyuesong.github.io/MagicArticulate">project page</a> /
									<a href="https://github.com/Seed3D/MagicArticulate">code</a> /
									<a href="https://arxiv.org/abs/2502.12135">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/classdiffusion.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://classdiffusion.github.io/">
										<span class="papertitle">ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance</span>
									</a>
									<br>
									Jiannan Huang,
									<strong>Jun Hao Liew</strong>,
									Hanshu Yan,
									Yuyang Yin,
									Yao Zhao,
									Yunchao Wei
									<br>
									<em>ICLR</em>, 2025
									<br>
									<a href="https://classdiffusion.github.io/">project page</a> /
									<a href="https://github.com/Rbrq03/ClassDiffusion">code</a> /
									<a href="https://arxiv.org/pdf/2405.17532">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicanimate-v2.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2409.19580">
										<span class="papertitle">High Quality Human Image Animation using Regional
											Supervision and Motion Blur Condition</span>
									</a>
									<br>
									Zhongcong Xu*, 
									Chaoyue Song*, 
									Guoxian Song*, 
									Jianfeng Zhang,
									<strong>Jun Hao Liew</strong>,
									Hongyi Xu,
									You Xie,
									Linjie Luo,
									Guosheng Lin,
									Jiashi Feng,
									Mike Zheng Shou
									<br>
									<em>arXiv</em>, 2024
									<br>
									<a href="https://arxiv.org/abs/2409.19580">arXiv</a>
									<p></p>
								</td>
							</tr>

							

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/creativity-vla.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/pdf/2406.00121">
										<span class="papertitle">Empowering Visual Creativity: A Vision-Language Assistant to Image Editing Recommendations</span>
									</a>
									<br>
									Tiancheng Shen,
									<strong>Jun Hao Liew</strong>,
									Long Mai,
									Lu Qi,
									Jiashi Feng,
									Jiaya Jia
									<br>
									<em>arXiv</em>, 2024
									<br>
									<!-- <a href="">project page</a> /
									<a href="">code</a> / -->
									<a href="https://arxiv.org/pdf/2406.00121">arXiv</a>
									<p></p>
								</td>
							</tr>


							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/perflow.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://piecewise-rectified-flow.github.io/">
										<span class="papertitle">PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator</span>
									</a>
									<br>
									Hanshu Yan,
									Xingchao Liu,
									Jiachun Pan,
									<strong>Jun Hao Liew</strong>,
									Qiang Liu,
									Jiashi Feng
									<br>
									<em>NeurIPS</em>, 2024
									<br>
									<a href="https://piecewise-rectified-flow.github.io/">project page</a> /
									<a href="https://github.com/magic-research/piecewise-rectified-flow">code</a> /
									<a href="https://arxiv.org/pdf/2405.07510">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicvideo-v2.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://magicvideov2.github.io/">
										<span class="papertitle">MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation</span>
									</a>
									<br>
									Weimin Wang*,
									Jiawei Liu*,
									Zhijie Lin,
									Jiangqiao Yan,
									Shuo Chen,
									Chetwin Low,
									<br>
									Tuyen Hoang,
									Jie Wu,
									<strong>Jun Hao Liew</strong>,
									Hanshu Yan,
									Daquan Zhou,
									Jiashi Feng
									<br>
									<em>arXiv</em>, 2024
									<br>
									<a href="https://magicvideov2.github.io/">project page</a> /
									<a href="https://arxiv.org/abs/2401.04468">arXiv</a>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/sag.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2312.12030">
										<span class="papertitle">Towards Accurate Guided Diffusion Sampling through Symplectic Adjoint Method</span>
									</a>
									<br>
									Jiachun Pan*,
									Hanshu Yan*,
									<strong>Jun Hao Liew</strong>,
									Jiashi Feng,
									Vincent V. F. Tan
									<br>
									<em>arXiv</em>, 2023
									<br>
									<!-- <a href="">project page</a> / -->
									<a href="https://github.com/HanshuYAN/AdjointDPM">code</a> /
									<a href="https://arxiv.org/abs/2312.12030">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/dragdiffusion.gif' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://yujun-shi.github.io/projects/dragdiffusion.html">
										<span class="papertitle">DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</span>
									</a>
									<br>
									Yujun Shi,
									Chuhui Xue,
									<strong>Jun Hao Liew</strong>,
									Jiachun Pan,
									Hanshu Yan,
									<br>
									Wenqing Zhang,
									Vincent Y. F. Tan,
									Song Bai
									<br>
									<em>CVPR</em>, 2024
									&nbsp;
									<font color="red">
										<strong>*Highlight</strong>
									</font>
									<br>
									<a href="https://yujun-shi.github.io/projects/dragdiffusion.html">project page</a> /
									<a href="https://github.com/Yujun-Shi/DragDiffusion">code</a> /
									<a href="https://arxiv.org/abs/2306.14435">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/avatarstudio.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="http://jeff95.me/projects/avatarstudio.html">
										<span class="papertitle">AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text</span>
									</a>
									<br>
									Jianfeng Zhang*,
									Xuanmeng Zhang*,
									Huichao Zhang,
									<strong>Jun Hao Liew</strong>,
									<br>
									Chenxu Zhang,
									Yi Yang,
									Jiashi Feng
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="http://jeff95.me/projects/avatarstudio.html">project page</a> /
									<a href="https://github.com/magic-research/avatarstudio">code</a> /
									<a href="https://arxiv.org/abs/2311.17917">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicanimate.gif' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://showlab.github.io/magicanimate">
										<span class="papertitle">MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</span>
									</a>
									<br>
									Zhongcong Xu,
									Jianfeng Zhang,
									<strong>Jun Hao Liew</strong>,
									Hanshu Yan,
									<br>
									Jia-Wei Liu,
									Chenxu Zhang,
									Jiashi Feng,
									Mike Zheng Shou
									<br>
									<em>CVPR</em>, 2024
									<br>
									<a href="https://showlab.github.io/magicanimate">project page</a> /
									<a href="https://github.com/magic-research/magic-animate">code</a> /
									<a href="https://arxiv.org/abs/2311.16498">arXiv</a> /
									<a href="https://huggingface.co/spaces/zcxu-eric/magicanimate">HuggingFace demo</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/xagen.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://showlab.github.io/xagen/">
										<span class="papertitle">XAGen: 3D Expressive Human Avatars Generation</span>
									</a>
									<br>
									Zhongcong Xu,
									Jianfeng Zhang,
									<strong>Jun Hao Liew</strong>,
									Jiashi Feng,
									Mike Zheng Shou
									<br>
									<em>NeurIPS</em>, 2023
									<br>
									<a href="https://showlab.github.io/xagen/">project page</a> /
									<a href="https://github.com/magic-research/xagen">code</a> /
									<a href="https://arxiv.org/abs/2311.13574">arXiv</a>
									<p></p>
								</td>
							</tr>
							
							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/mixval.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openreview.net/forum?id=ackajXqei2">
										<span class="papertitle">Mixed Samples as Probes for Unsupervised Model Selection in Domain Adaptation</span>
									</a>
									<br>
									Dapeng Hu,
									Jian Liang,
									<strong>Jun Hao Liew</strong>,
									Chuihui Xue,
									Song Bai,
									Xiaochang Wang
									<br>
									<em>NeurIPS</em>, 2023
									<br>
									<a href="https://github.com/LHXXHB/MixVal">code</a> /
									<a href="https://openreview.net/forum?id=ackajXqei2">paper</a>
									<p>
									</p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/segrefiner.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2312.12425">
										<span class="papertitle">SegRefiner: Towards Model-Agnostic Segmentation Refinement with Discrete Diffusion Process</span>
									</a>
									<br>
									Mengyu Wang,
									Henghui Ding,
									<strong>Jun Hao Liew</strong>, 
									Jiajun Liu,
									Yao Zhao,
									Yunchao Wei
									<br>
									<em>NeurIPS</em>, 2023
									<br>
									<a href="https://github.com/MengyuWang826/SegRefiner">code</a> /
									<a href="https://arxiv.org/abs/2312.12425">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicedit.gif' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://magic-edit.github.io/">
										<span class="papertitle">MagicEdit: High-Fidelity Temporally Coherent Video Editing</span>
									</a>
									<br>
									<strong>Jun Hao Liew*</strong>, 
									Hanshu Yan*,
									Jianfeng Zhang,
									Zhongcong Xu,
									Jiashi Feng
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://magic-edit.github.io/">project page</a> /
									<a href="https://github.com/magic-research/magic-edit">code</a> /
									<a href="https://arxiv.org/abs/2308.14749">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicavatar.gif' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://magic-avatar.github.io/">
										<span class="papertitle">MagicAvatar: Multimodal Avatar Generation and Animation</span>
									</a>
									<br>
									Jianfeng Zhang*,
									Hanshu Yan*,
									Zhongcong Xu*,
									Jiashi Feng,
									<strong>Jun Hao Liew*</strong>
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://magic-avatar.github.io/">project page</a> /
									<a href="https://github.com/magic-research/magic-avatar">code</a> /
									<a href="https://arxiv.org/abs/2308.14748">arXiv</a> /
									<a href="https://youtu.be/UN7W5oKmWNA">youtube</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/magicprop.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2309.00908">
										<span class="papertitle">MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation</span>
									</a>
									<br>
									Hanshu Yan*,
									<strong>Jun Hao Liew*</strong>
									Long Mai,
									Shanchuan Lin,
									Jiashi Feng									
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://arxiv.org/abs/2309.00908">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/gkc.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2303.09181">
										<span class="papertitle">Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</span>
									</a>
									<br>
									Kunyang Han*,
									Yong Liu*,
									<strong>Jun Hao Liew</strong>, 
									Henghui Ding,
									Yunchao Wei,
									<br>
									Jiajun Liu,
									Yitong Wang,
									Yansong Tang,
									Jiashi Feng,
									Yao Zhao
									<br>
									<em>ICCV</em>, 2023
									<br>
									<a href="https://arxiv.org/abs/2303.09181">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									  <img src='images/adjointdpm.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2307.10711">
									  <span class="papertitle">AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models</span>
									</a>
									<br>
									Jiachun Pan*,
									<strong>Jun Hao Liew</strong>,
									Vincent Y. F. Tan,
									Jiashi Feng,
									Hanshu Yan*
									<br>
									<em>ICLR</em>, 2024
									<br>
									<a">project page</a>
									/
									<a href="https://arxiv.org/abs/2307.10711">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/data_scaling.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2305.15248">
										<span class="papertitle">Delving Deeper into Data Scaling in Masked Image Modeling</span>
									</a>
									<br>
									Cheng-Ze Lu,
									Xiaojie Jin,
									Qibin Hou,
									<strong>Jun Hao Liew</strong>,
									Ming-Ming Cheng,
									Jiashi Feng 
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://arxiv.org/abs/2305.15248">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									  <img src='images/associate.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2304.01114">
									<span class="papertitle">Associating Spatially-Consistent Grouping with Text-supervised Semantic Segmentation</span>
									</a>
									<br>
									Yabo Zhang,
									Zihao Wang,
									<strong>Jun Hao Liew</strong>, 
									Jingjia Huang,
									Manyu Zhu, <br>
									Jiashi Feng,
									Wangmeng Zuo
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://arxiv.org/abs/2304.01114">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									  <img src='images/pv3d.gif' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://showlab.github.io/pv3d/">
										<span class="papertitle">PV3D: A 3D Generative Model for Portrait Video Generation</span>
									</a>
									<br>
									Zhongcong Xu,
									Jianfeng Zhang,
									<strong>Jun Hao Liew</strong>, 
									Wenqing Zhang,<br>
									Song Bai,
									Jiashi Feng,
									Mike Zheng Shou
									<br>
									<em>ICLR</em>, 2023
									<br>
									<a href="https://showlab.github.io/pv3d/">project page</a> /
									<a href="https://github.com/bytedance/pv3d">code</a> /
									<a href="https://arxiv.org/abs/2212.06384">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle;">
									  <img src='images/magicmix_.png' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://magicmix.github.io/">
										<span class="papertitle">MagicMix: Semantic Mixing with Diffusion Models</span>
									</a>
									<br>
									<strong>Jun Hao Liew*</strong>, 
									Hanshu Yan*,
									Daquan Zhou,
									Jiashi Feng
									<br>
									<em>arXiv</em>, 2023
									<br>
									<a href="https://magicmix.github.io/">project page</a> /
									<a href="https://arxiv.org/abs/2210.16056">arXiv</a> / 
									<a href="https://github.com/huggingface/diffusers/blob/main/examples/community/README.md#magic-mix">code (diffusers)</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src='images/slimscissors.jpg' width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://kunyanghan.github.io/SlimScissors/">
										<span class="papertitle">Slim Scissors: Segmenting Thin Object from Synthetic Background</span>
									</a>
									<br>
									Kunyang Han,
									<strong>Jun Hao Liew </strong>,
									Jiashi Feng,
									Huawei Tian,
									Yao Zhao,
									Yunchao Wei
									<br>
									<em>ECCV</em>, 2022
									<br>
									<a href="https://kunyanghan.github.io/SlimScissors/">project page</a> /
									<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890375.pdf">paper</a> /
									<a href="https://github.com/KunyangHan/SlimScissors">code</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/sodar.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/2304.14473">
									<span class="papertitle">SODAR: Segmenting Objects by Dynamically Aggregating Neighboring Mask Representations</span>
									</a>
									<br>
									Tao Wang,
									<strong>Jun Hao Liew  </strong>,
									Yu Li,
									Yunpeng Chen,
									Jiashi Feng
									<br>
									<em>TIP</em>, 2021
									<br>
									<a href="https://arxiv.org/pdf/2202.07402.pdf">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/cfpn.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://ieeexplore.ieee.org/abstract/document/9408384">
									<span class="papertitle">Cross-layer feature pyramid network for salient object detection</span>
									</a>
									<br>
									Zun Li,
									Congyan Lang, 
									<strong>Jun Hao Liew  </strong>,
									Yidong Li,
									Qibin Hou,
									Jiashi Feng
									<br>
									<em>TIP</em>, 2021
									<br>
									<a href="https://arxiv.org/abs/2002.10864">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/bmp.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Body_Meshes_as_Points_CVPR_2021_paper.html">
									<span class="papertitle">Body meshes as points</span>
									</a>
									<br>
									Jianfeng Zhang,
									Dongdong Yu,
									<strong>Jun Hao Liew  </strong>,
									Xuecheng Nie,
									Jiashi Feng
									<br>
									<em>CVPR</em>, 2021
									<br>
									<a href="https://arxiv.org/abs/2105.02467">arXiv</a> /
									<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhang_Body_Meshes_as_CVPR_2021_supplemental.pdf">supp</a> /
									<a href="https://github.com/jfzhang95/BMP/">code</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/superpixel.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.html">
									<span class="papertitle">Revisiting Superpixels for Active Learning in Semantic Segmentation With Realistic Annotation Costs</span>
									</a>
									<br>
									Lile Cai,
									Xun Xu,
									<strong>Jun Hao Liew  </strong>,
									Chuan Sheng Foo
									<br>
									<em>CVPR</em>, 2021
									<br>
									<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Cai_Revisiting_Superpixels_for_CVPR_2021_supplemental.pdf">supp</a> /
									<a href="https://github.com/cailile/Revisiting-Superpixels-for-Active-Learning">code</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/dance.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content/WACV2021/html/Liu_DANCE_A_Deep_Attentive_Contour_Model_for_Efficient_Instance_Segmentation_WACV_2021_paper.html">
									<span class="papertitle">DANCE: A Deep Attentive Contour Model for Efficient Instance Segmentation</span>
									</a>
									<br>
									Zichen Liu*,
									<strong>Jun Hao Liew*</strong>,
									Xiangyu Chen,
									Jiashi Feng
									<br>
									<em>WACV</em>, 2021
									<br>
									<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Liu_DANCE_A_Deep_Attentive_Contour_Model_for_Efficient_Instance_Segmentation_WACV_2021_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content/WACV2021/supplemental/Liu_DANCE_A_Deep_WACV_2021_supplemental.pdf">supp</a> /
									<a href="https://github.com/lkevinzc/dance">code</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/tos.gif" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content/WACV2021/html/Liew_Deep_Interactive_Thin_Object_Selection_WACV_2021_paper.html">
									<span class="papertitle">Deep Interactive Thin Object Selection</span>
									</a>
									<br>
									<strong>Jun Hao Liew  </strong>,
									Scott Cohen,
									Brian Price,
									Long Mai,
									Jiashi Feng
									<br>
									<em>WACV</em>, 2021
									<br>
									<a href="https://openaccess.thecvf.com/content/WACV2021/papers/Liew_Deep_Interactive_Thin_Object_Selection_WACV_2021_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content/WACV2021/supplemental/Liew_Deep_Interactive_Thin_WACV_2021_supplemental.pdf">supp</a> /
									<a href="https://github.com/liewjunhao/thin-object-selection">code</a> /
									<a href="https://drive.google.com/file/d/1Hiwi7hXFIifMis4q9YOwj1fa2K6OAHZk/view?usp=sharing">ThinObject-5K dataset</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/devil.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/pdf/2007.11978">
									<span class="papertitle">The devil is in classification: A simple framework for long-tail instance segmentation</span>
									</a>
									<br>
									Tao Wang, 
									Yu Li,
									Bingyi Kang,
									Junnan Li,
									<strong>Jun Hao Liew</strong>,
									Sheng Tang,
									Steven Hoi,
									Jiashi Feng
									<br>
									<em>ECCV</em>, 2020 &nbsp;
									<font color="red">
										<strong>*LVIS 2019 winner</strong>
									</font>
									<br>
									<a href="https://arxiv.org/pdf/2007.11978">arXiv</a> /
									<a href="https://github.com/twangnh/SimCal">code</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/iog.gif" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://github.com/shiyinzhang/Inside-Outside-Guidance">
									<span class="papertitle">Interactive Object Segmentation With Inside-Outside Guidance</span>
									</a>
									<br>
									Shiyin Zhang,
									<strong>Jun Hao Liew  </strong>,
									Yunchao Wei,
									Shikui Wei,
									Yao Zhao,
									Jiashi Feng
									<br>
									<em>CVPR</em>, 2020 &nbsp;
									<font color="red">
										<strong>*Oral presentation</strong>
									</font>
									<br>
									<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Interactive_Object_Segmentation_With_Inside-Outside_Guidance_CVPR_2020_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Zhang_Interactive_Object_Segmentation_CVPR_2020_supplemental.zip">supp</a> /
									<a href="https://github.com/shiyinzhang/Inside-Outside-Guidance">code</a> /
									<a href="https://github.com/shiyinzhang/Pixel-ImageNet">Pixel-ImageNet dataset</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/salient_deep_reasoning.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://arxiv.org/abs/1901.08362">
									<span class="papertitle">Deep Reasoning with Multi-scale Context for Salient Object Detection</span>
									</a>
									<br>
									Zun Li,
									Congyan Lang,
									Yunpeng Chen,
									<strong>Jun Hao Liew  </strong>,
									Jiashi Feng
									<br>
									<em>arXiv</em>, 2019
									<br>
									<a href="https://arxiv.org/abs/1901.08362">arXiv</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/multiseg.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.pdf">
									<span class="papertitle">MultiSeg: Semantically Meaningful, Scale-Diverse Segmentations From Minimal User Input</span>
									</a>
									<br>
									<strong>Jun Hao Liew  </strong>,
									Scott Cohen,
									Brian Price,
									Long Mai,
									Sim-Heng Ong,
									Jiashi Feng
									<br>
									<em>ICCV</em>, 2019
									<br>
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liew_MultiSeg_Semantically_Meaningful_Scale-Diverse_Segmentations_From_Minimal_User_Input_ICCV_2019_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Liew_MultiSeg_Semantically_Meaningful_ICCV_2019_supplemental.pdf">supp</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/panet.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf">
									<span class="papertitle">PANet: Few-Shot Image Semantic Segmentation with Prototype Alignment</span>
									</a>
									<br>
									Kaixin Wang,
									<strong>Jun Hao Liew  </strong>,
									Yingtian Zhou,
									Daquan Zhou,
									Jiashi Feng
									<br>
									<em>ICCV</em>, 2019 &nbsp;
									<font color="red">
										<strong>*Oral presentation</strong>
									</font>
									<br>
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_PANet_Few-Shot_Image_Semantic_Segmentation_With_Prototype_Alignment_ICCV_2019_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Wang_PANet_Few-Shot_Image_ICCV_2019_supplemental.pdf">supp</a> /
									<a href="https://github.com/kaixin96/PANet">code</a> /
									<a href="https://www.youtube.com/watch?v=2ntDYowHbZs&t=1459s">video</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/fsenet.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xuan_Chen_Focus_Segment_and_ECCV_2018_paper.pdf">
									<span class="papertitle">Focus, Segment and Erase: An Efficient Network for Multi-label Brain Tumor Segmentation</span>
									</a>
									<br>
									Xuan Chen*,
									<strong>Jun Hao Liew*</strong>,
									Wei Xiong,
									Chee-Kong Chui,
									Sim-Heng Ong,
									<br>
									<em>ECCV</em>, 2018
									<br>
									<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Xuan_Chen_Focus_Segment_and_ECCV_2018_paper.pdf">paper</a>
									<p></p>
								</td>
							</tr>

							<tr>
								<td style="padding:20px;width:25%;vertical-align:middle">
									<img src="images/risnet.png" width="160" height="120">
								</td>
								<td style="padding:20px;width:75%;vertical-align:middle">
									<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Liew_Regional_Interactive_Image_ICCV_2017_paper.pdf">
									<span class="papertitle">Regional Interactive Image Segmentation Networks</span>
									</a>
									<br>
									<strong>Jun Hao Liew  </strong>,
									Yunchao Wei,
									Wei Xiong,
									Sim-Heng Ong,
									Jiashi Feng
									<br>
									<em>ICCV</em>, 2017
									<br>
									<a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Liew_Regional_Interactive_Image_ICCV_2017_paper.pdf">paper</a> /
									<a href="https://openaccess.thecvf.com/content_ICCV_2017/supplemental/Liew_Regional_Interactive_Image_ICCV_2017_supplemental.pdf">supp</a>
									<p></p>
								</td>
							</tr>
			
						</tbody>
					</table>
				</td>
			</tr>
		</table>

	<p style="text-align:center;">Special thanks to <a href="https://jonbarron.info/">Jon Barron</a> for the website template.</p>
	</body>
</html>